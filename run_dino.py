import torch

# vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')
# vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')

# vitb8 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')
# xcit_small_12_p16 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p16')
# xcit_small_12_p8 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p8')
# xcit_medium_24_p16 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p16')
# xcit_medium_24_p8 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p8')
# resnet50 = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')
# from models_vit import vit_base_patch16
import os
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import torch.nn as nn
from model import save_model
import argparse
from dataset import Dataset
from run_finetune import yaml_config_hook, cal_metric, train, val
from utils import make_df, write_result

# Copyright (c) Facebook, Inc. and its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Mostly copy-paste from timm library.
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
"""
import math
from functools import partial

import torch
import torch.nn as nn


def refine_keys(ckpt):
    from collections import OrderedDict
    new_state_dict = OrderedDict()
    for k, v in ckpt.items():
        if 'pos_embed' in k:
            pass
        else:
            name = k[:]
            new_state_dict[name] = v
    return new_state_dict


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

#
# def drop_path(x, drop_prob: float = 0., training: bool = False):
#     if drop_prob == 0. or not training:
#         return x
#     keep_prob = 1 - drop_prob
#     shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
#     random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
#     random_tensor.floor_()  # binarize
#     output = x.div(keep_prob) * random_tensor
#     return output
#
#
# class DropPath(nn.Module):
#     """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
#     """
#
#     def __init__(self, drop_prob=None):
#         super(DropPath, self).__init__()
#         self.drop_prob = drop_prob
#
#     def forward(self, x):
#         return drop_path(x, self.drop_prob, self.training)
#
#
# class Mlp(nn.Module):
#     def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
#         super().__init__()
#         out_features = out_features or in_features
#         hidden_features = hidden_features or in_features
#         self.fc1 = nn.Linear(in_features, hidden_features)
#         self.act = act_layer()
#         self.fc2 = nn.Linear(hidden_features, out_features)
#         self.drop = nn.Dropout(drop)
#
#     def forward(self, x):
#         x = self.fc1(x)
#         x = self.act(x)
#         x = self.drop(x)
#         x = self.fc2(x)
#         x = self.drop(x)
#         return x
#
#
# class Attention(nn.Module):
#     def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
#         super().__init__()
#         self.num_heads = num_heads
#         head_dim = dim // num_heads
#         self.scale = qk_scale or head_dim ** -0.5
#
#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
#         self.attn_drop = nn.Dropout(attn_drop)
#         self.proj = nn.Linear(dim, dim)
#         self.proj_drop = nn.Dropout(proj_drop)
#
#     def forward(self, x):
#         B, N, C = x.shape
#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
#         q, k, v = qkv[0], qkv[1], qkv[2]
#
#         attn = (q @ k.transpose(-2, -1)) * self.scale
#         attn = attn.softmax(dim=-1)
#         attn = self.attn_drop(attn)
#
#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)
#         x = self.proj(x)
#         x = self.proj_drop(x)
#         return x, attn
#
#
# class Block(nn.Module):
#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
#                  drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
#         super().__init__()
#         self.norm1 = norm_layer(dim)
#         self.attn = Attention(
#             dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
#         self.norm2 = norm_layer(dim)
#         mlp_hidden_dim = int(dim * mlp_ratio)
#         self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
#
#     def forward(self, x, return_attention=False):
#         y, attn = self.attn(self.norm1(x))
#         if return_attention:
#             return attn
#         x = x + self.drop_path(y)
#         x = x + self.drop_path(self.mlp(self.norm2(x)))
#         return x
#
#
# class PatchEmbed(nn.Module):
#     """ Image to Patch Embedding
#     """
#
#     def __init__(self, img_size=(121, 145, 121), patch_size=16, in_chans=3, embed_dim=768):
#         super().__init__()
#         num_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size) * (img_size[2] // patch_size)
#         self.img_size = img_size
#         self.patch_size = patch_size
#         self.num_patches = num_patches
#
#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
#
#     def forward(self, x):
#         B, C, H, W, D = x.shape
#         x = self.proj(x).flatten(2).transpose(-1, -2)
#         return x
#
#
# class VisionTransformer(nn.Module):
#     """ Vision Transformer """
#
#     def __init__(self, img_size=(121, 145, 121), patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,
#                  num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
#                  drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):
#         super().__init__()
#         self.num_features = self.embed_dim = embed_dim
#
#         self.patch_embed = PatchEmbed(
#             img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
#         num_patches = self.patch_embed.num_patches
#
#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
#         self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
#         self.pos_drop = nn.Dropout(p=drop_rate)
#
#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
#         self.blocks = nn.ModuleList([
#             Block(
#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
#             for i in range(depth)])
#         self.norm = norm_layer(embed_dim)
#
#         # Classifier head
#         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
#
#         trunc_normal_(self.pos_embed, std=.02)
#         trunc_normal_(self.cls_token, std=.02)
#         self.apply(self._init_weights)
#
#     def _init_weights(self, m):
#         if isinstance(m, nn.Linear):
#             trunc_normal_(m.weight, std=.02)
#             if isinstance(m, nn.Linear) and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)
#         elif isinstance(m, nn.LayerNorm):
#             nn.init.constant_(m.bias, 0)
#             nn.init.constant_(m.weight, 1.0)
#
#     def interpolate_pos_encoding(self, x, w, h, d):
#         npatch = x.shape[1] - 1
#         N = self.pos_embed.shape[1] - 1
#         if npatch == N and w == h:
#             return self.pos_embed
#         class_pos_embed = self.pos_embed[:, 0]
#         patch_pos_embed = self.pos_embed[:, 1:]
#         dim = x.shape[-1]
#         w0 = w // self.patch_embed.patch_size
#         h0 = h // self.patch_embed.patch_size
#         d0 = d // self.patch_embed.patch_size
#         # we add a small number to avoid floating point error in the interpolation
#         # see discussion at https://github.com/facebookresearch/dino/issues/8
#         w0, h0, d0 = w0 + 0.1, h0 + 0.1, d0 + 0.1
#         patch_pos_embed = nn.functional.interpolate(
#             patch_pos_embed.reshape(1, 7, 9, 7, dim).permute(0, 3, 1, 4, 2),
#             scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N), d0 / math.sqrt(N)),
#             mode='trilinear',
#         )
#         # assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
#         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)
#         return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)
#
#     def prepare_tokens(self, x):
#         B, nc, w, h, d = x.shape
#         x = self.patch_embed(x)  # patch linear embedding
#
#         # add the [CLS] token to the embed patch tokens
#         cls_tokens = self.cls_token.expand(B, -1, -1)
#         x = torch.cat((cls_tokens, x), dim=1)
#
#         # add positional encoding to each token
#         x = x + self.interpolate_pos_encoding(x, w, h, d)
#
#         return self.pos_drop(x)
#
#     def forward(self, x):
#         x = self.prepare_tokens(x)
#         for blk in self.blocks:
#             x = blk(x)
#         x = self.norm(x)
#         return x[:, 0]
#
#     def get_last_selfattention(self, x):
#         x = self.prepare_tokens(x)
#         for i, blk in enumerate(self.blocks):
#             if i < len(self.blocks) - 1:
#                 x = blk(x)
#             else:
#                 # return attention of the last block
#                 return blk(x, return_attention=True)
#
#     def get_intermediate_layers(self, x, n=1):
#         x = self.prepare_tokens(x)
#         # we return the output tokens from the `n` last blocks
#         output = []
#         for i, blk in enumerate(self.blocks):
#             x = blk(x)
#             if len(self.blocks) - i <= n:
#                 output.append(self.norm(x))
#         return output

from models_vit import vit_base_patch16
def vit_tiny(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_small(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_base(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048,
                 bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x


def main(gpu, args):
    for fold in range(5):
        train_dataset = Dataset(dataset='train', fold=fold, args=args)
        val_dataset = Dataset(dataset='validation', fold=fold, args=args)

        train_sampler = None
        val_sampler = None

        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=(train_sampler is None),
            drop_last=True,
            num_workers=args.workers,
            sampler=train_sampler,
            pin_memory=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=(val_sampler is None),
            drop_last=True,
            num_workers=args.workers,
            sampler=val_sampler,
            pin_memory=True
        )

        # initialize model
        model = vit_base(patch_size=16)
        ckpt = torch.load('dino_vitbase16_pretrain.pth')
        model.load_state_dict(refine_keys(ckpt), strict=False)
        model.patch_embed.proj = nn.Conv3d(in_channels=1, out_channels=768, kernel_size=16, stride=16)

        if (args.device.type == 'cuda') and (torch.cuda.device_count() > 1):
            print("Multi GPU ACTIVATES")
            model = nn.DataParallel(model)
        model = model.to(args.device)

        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
        if args.scheduler:
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', 0.5, patience=5, verbose=True,
                                                                   cooldown=10)
        criterion = torch.nn.CrossEntropyLoss()

        model = model.to(args.device)

        writer = None
        if args.nr == 0:
            writer = SummaryWriter()

        args.global_step = 0
        args.current_epoch = 0
        result_df = make_df(finetune=args.finetune)

        for epoch in range(args.start_epoch, args.epochs):
            model.train()
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)

            lr = optimizer.param_groups[0]['lr']
            loss_epoch, train_cf_list = train(args, train_loader, model, criterion, optimizer, writer)
            train_acc, train_bacc, train_spe, train_sen, train_f1, train_prec = cal_metric(train_cf_list)
            # if args.nr == 0 and scheduler:
            #     scheduler.step()

            # if args.nr == 0 and epoch % 30 == 0:
            #     save_model(args, model, optimizer)

            # if args.nr == 0:
            #     writer.add_scalar("Loss/train", loss_epoch / len(train_loader), epoch)
            #     writer.add_scalar("Misc/learning_rate", lr, epoch)
            #     print(
            #         f"Epoch [{epoch}/{args.epochs}]\t Loss: {loss_epoch / len(train_loader)}\t lr: {round(lr, 5)}"
            #     )
            #     args.current_epoch += 1

            with torch.no_grad():
                model.eval()
                val_loss_epoch, val_cf_list = val(args, val_loader, model, criterion, optimizer, writer)
                val_acc, val_bacc, val_spe, val_sen, val_f1, val_prec = cal_metric(val_cf_list)
                # accuracy, balnced accuracy, specificity, sensitivity, F1, precision
                print('\n')
                print('\n')
                print(f"Fold {fold}: epoch : {epoch}")
                print('\n')
                print(f'Loss : Train = {loss_epoch} | Val = {val_loss_epoch} ')
                print('\n')
                print("Train")
                print('\n')
                print(f'ACC : {train_acc}% | F1 : {train_f1}')
                print(f'BACC : {train_bacc}%')
                print(f'Specificity : {train_spe} | Sensitivity : {train_sen}')
                print(f'Precision : {train_prec}')
                print('\n')
                print("Validation")
                print('\n')
                print(f'ACC : {val_acc}% | F1 : {val_f1}')
                print(f'BACC : {val_bacc}%')
                print(f'Specificity : {val_spe} | Sensitivity : {val_sen}')
                print(f'Precision : {val_prec}')
            if args.scheduler:
                scheduler.step(train_acc)

            output_list = [epoch, loss_epoch, val_loss_epoch, train_acc, val_acc, train_bacc, val_bacc, train_spe,
                           val_spe, train_sen, val_sen, train_f1, val_f1, train_prec, val_prec]
            result_df = write_result(output_list, result_df)
        save_model(args, model, optimizer)
        result_df.to_csv(f'{args.model_path}/{args.modality}_mci_{args.mci}_results_fold_{fold}.csv')


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="SimCLR")
    parser.add_argument('--config', default='./config/finetune.yaml', type=str)

    args = parser.parse_args()
    config = yaml_config_hook(args.config)

    for k, v in config.items():
        parser.add_argument(f"--{k}", default=v, type=type(v))
    args = parser.parse_args()

    for arg in vars(args):
        print(f'--{arg}', getattr(args, arg))

    # Master address for distributed data parallel
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"] = "8000"

    if not os.path.exists(args.model_path):
        os.makedirs(args.model_path)

    args.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    args.num_gpus = torch.cuda.device_count()
    args.world_size = args.gpus * args.nodes

    main(0, args)
